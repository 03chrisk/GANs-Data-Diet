{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST GAN Implementation\n",
    "\n",
    "This notebook implements a Generative Adversarial Network (GAN) trained on the MNIST dataset using PyTorch. This serves as the foundation for investigating whether informative data samples exist for GAN training, similar to those found in supervised learning.\n",
    "\n",
    "## Thesis Context\n",
    "Recent research has shown that state-of-the-art machine learning models can be trained effectively using only specific subsets of training data. These \"informative samples\" enhance generalization in neural networks. While this phenomenon has been observed in supervised learning, this project aims to investigate whether similar informative samples exist when training generative models like GANs.\n",
    "\n",
    "## Notebook Overview\n",
    "1. **Setup and Imports**: Libraries and device configuration\n",
    "2. **Hyperparameters**: Configuration for the GAN training\n",
    "3. **Model Architecture**: Implementation of Generator and Discriminator networks\n",
    "4. **Data Loading**: MNIST dataset preparation\n",
    "5. **Visualization Functions**: Utilities for tracking GAN performance\n",
    "6. **Training Loop**: Implementation of the GAN training process\n",
    "7. **Execution and Results**: Running the training and analyzing outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up the computational device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters\n",
    "\n",
    "Here we define all the hyperparameters that control the GAN training process. These include:\n",
    "\n",
    "- Batch size: Number of images processed in each training step\n",
    "- Latent dimension: Size of the random noise vector input to the generator\n",
    "- Hidden dimensions: Size of hidden layers in the networks\n",
    "- Learning rates and optimizer parameters\n",
    "- Training duration and image sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 128     # Size of generator input noise vector\n",
    "hidden_dim = 256     # Size of hidden layers\n",
    "image_size = 28 * 28 # MNIST image dimensions flattened\n",
    "lr = 0.0002          # Learning rate\n",
    "beta1 = 0.5          # Adam optimizer beta1\n",
    "beta2 = 0.999        # Adam optimizer beta2\n",
    "num_epochs = 100     # Number of training epochs\n",
    "sample_interval = 10 # Save images every 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### Generator Network\n",
    "\n",
    "The Generator transforms random noise vectors into synthetic images. Its architecture consists of:\n",
    "- Input: Random noise vector (latent_dim)\n",
    "- Multiple fully connected layers with LeakyReLU activations\n",
    "- Output: Image with values in range [-1, 1] through Tanh activation\n",
    "\n",
    "This implementation uses a simple MLP (Multi-Layer Perceptron) architecture rather than convolutional layers for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(hidden_dim * 4, output_dim),\n",
    "            nn.Tanh()  # Output values between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)  # Reshape to image dimensions\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "\n",
    "The Discriminator evaluates whether an image is real (from the dataset) or fake (generated). Its architecture consists of:\n",
    "- Input: Flattened image (28Ã—28 = 784 dimensions)\n",
    "- Multiple fully connected layers with LeakyReLU activations and dropout for regularization\n",
    "- Output: Single value between 0-1 through Sigmoid activation (probability of image being real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output between 0-1 (probability of being real)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten the image\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Models and Optimizers\n",
    "\n",
    "Here we:\n",
    "1. Initialize both networks\n",
    "2. Define the loss function (Binary Cross Entropy)\n",
    "3. Set up optimizers for both networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(latent_dim, hidden_dim, image_size).to(device)\n",
    "discriminator = Discriminator(image_size, hidden_dim).to(device)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Here we set up the MNIST dataset with appropriate transformations:\n",
    "- ToTensor: Converts PIL images to PyTorch tensors\n",
    "- Normalize: Scales pixel values from [0,1] to [-1,1] to match the Generator's Tanh output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "def load_data(subset_percentage=100):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    full_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    all_targets = full_dataset.targets.numpy()\n",
    "    \n",
    "    digit_counts = [0] * 10\n",
    "    for label in all_targets:\n",
    "        digit_counts[label] += 1\n",
    "    \n",
    "    print(\"Full dataset distribution:\")\n",
    "    for digit, count in enumerate(digit_counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    \n",
    "    if subset_percentage == 100:\n",
    "        selected_dataset = full_dataset\n",
    "    else:\n",
    "        digit_indices = [[] for _ in range(10)]\n",
    "        \n",
    "        for idx, label in enumerate(all_targets):\n",
    "            digit_indices[label].append(idx)\n",
    "        \n",
    "        total_subset_size = int(len(full_dataset) * subset_percentage / 100)\n",
    "        samples_per_digit = total_subset_size // 10\n",
    "        \n",
    "        stratified_indices = []\n",
    "        for digit in range(10):\n",
    "            digit_idx = digit_indices[digit]\n",
    "            random_idx = torch.randperm(len(digit_idx))\n",
    "\n",
    "            selected_idx = [digit_idx[i] for i in random_idx[:samples_per_digit]]\n",
    "            stratified_indices.extend(selected_idx)\n",
    "        print(all_targets[stratified_indices])\n",
    "        random.shuffle(stratified_indices)\n",
    "        print(all_targets[stratified_indices])\n",
    "        selected_dataset = torch.utils.data.Subset(full_dataset, stratified_indices)\n",
    "        \n",
    "        subset_digit_counts = [0] * 10\n",
    "        for idx in stratified_indices:\n",
    "            label = all_targets[idx]\n",
    "            subset_digit_counts[label] += 1\n",
    "        \n",
    "        print(\"\\nStratified subset distribution:\")\n",
    "        for digit, count in enumerate(subset_digit_counts):\n",
    "            print(f\"Digit {digit}: {count} samples\")\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        selected_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True  # Discard incomplete batches\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFull dataset size: {len(full_dataset)} images\")\n",
    "    print(f\"Selected subset size: {len(selected_dataset)} images\")\n",
    "    print(f\"Number of batches: {len(data_loader)}\")\n",
    "    \n",
    "    return data_loader, selected_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "subset_percentage = 90\n",
    "train_loader, train_dataset = load_data(subset_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions\n",
    "\n",
    "These utility functions help monitor the GAN's progress:\n",
    "\n",
    "1. `save_generated_images`: Creates and saves a grid of generated images\n",
    "2. `plot_losses`: Visualizes generator and discriminator losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_images(epoch, generator, latent_dim, device, subset_percentage, \n",
    "                         fixed_noise=None, base_path=\"../generated_images\"):\n",
    "    \n",
    "    subfolder = f\"subset_{subset_percentage}_percent\"\n",
    "    save_path = os.path.join(base_path, subfolder)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Create a batch of latent vectors or use fixed noise for comparison\n",
    "    if fixed_noise is None:\n",
    "        z = torch.randn(25, latent_dim).to(device)\n",
    "    else:\n",
    "        z = fixed_noise\n",
    "    \n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z).detach().cpu()\n",
    "    \n",
    "    # Rescale images from [-1, 1] to [0, 1]\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    grid = make_grid(gen_imgs, nrow=5, normalize=True)\n",
    "    \n",
    "    filename = f\"epoch_{epoch:03d}.png\"\n",
    "    filepath = os.path.join(save_path, filename)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Generated Images - {subset_percentage}% Data - Epoch {epoch}\")\n",
    "\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(g_losses, d_losses, subset_percentage, save_path=\"../loss_plots\"):\n",
    "    # Create folder structure\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f'GAN Training Losses - {subset_percentage}% Data')\n",
    "    \n",
    "    plt.savefig(f\"{save_path}/losses_subset_{subset_percentage}_percent.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "The GAN training function implements the adversarial training process with these key steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Create fixed noise vector for consistent image generation across epochs\n",
    "   - Initialize lists to track losses\n",
    "\n",
    "2. **For each epoch and batch**:\n",
    "   - **Train Discriminator**:\n",
    "     - Forward pass with real images â†’ calculate loss on real images\n",
    "     - Generate fake images â†’ calculate loss on fake images\n",
    "     - Update discriminator weights\n",
    "   - **Train Generator**:\n",
    "     - Generate fake images\n",
    "     - Calculate loss based on discriminator's prediction\n",
    "     - Update generator weights\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Save losses for plotting\n",
    "   - Generate and save images at regular intervals\n",
    "   - Track and display progress information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(subset_percentage=100):\n",
    "    # Generate fixed noise for consistent image generation\n",
    "    fixed_noise = torch.randn(25, latent_dim).to(device)\n",
    "    \n",
    "    # Create lists to store losses\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        g_loss_epoch = 0\n",
    "        d_loss_epoch = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i, (real_imgs, _) in enumerate(train_loader):\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Configure input\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            \n",
    "            # Create labels with a small amount of label smoothing\n",
    "            valid = torch.ones(batch_size, 1).to(device) * 0.9\n",
    "            fake = torch.zeros(batch_size, 1).to(device) + 0.1\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Loss on real images\n",
    "            real_pred = discriminator(real_imgs)\n",
    "            d_real_loss = adversarial_loss(real_pred, valid)\n",
    "            \n",
    "            # Sample noise and generate fake images\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            \n",
    "            # Loss on fake images\n",
    "            fake_pred = discriminator(fake_imgs.detach())\n",
    "            d_fake_loss = adversarial_loss(fake_pred, fake)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            \n",
    "            # Try to fool the discriminator\n",
    "            validity = discriminator(fake_imgs)\n",
    "            g_loss = adversarial_loss(validity, valid)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Save losses for plotting\n",
    "            g_loss_epoch += g_loss.item()\n",
    "            d_loss_epoch += d_loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] \"\n",
    "                      f\"[Time: {time.time() - start_time:.2f}s]\")\n",
    "        \n",
    "        # Calculate and store average losses for this epoch\n",
    "        g_losses.append(g_loss_epoch / batch_count)\n",
    "        d_losses.append(d_loss_epoch / batch_count)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] \"\n",
    "              f\"[Avg D loss: {d_losses[-1]:.4f}] [Avg G loss: {g_losses[-1]:.4f}]\")\n",
    "\n",
    "        \n",
    "        # Save generated images at specified intervals\n",
    "        if epoch % sample_interval == 0 or epoch == num_epochs - 1:\n",
    "            _ = save_generated_images(epoch, generator, latent_dim, device, subset_percentage, fixed_noise)\n",
    "            \n",
    "        # Plot losses at each epoch\n",
    "        if epoch % 10 == 0:\n",
    "            plot_losses(g_losses, d_losses, subset_percentage)\n",
    "    \n",
    "    plot_losses(g_losses, d_losses, subset_percentage)\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "    \n",
    "    # Also update the model saving to include subset percentage\n",
    "    model_path = f\"../models/subset_{subset_percentage}_percent\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(generator.state_dict(), f'{model_path}/generator.pth')\n",
    "    torch.save(discriminator.state_dict(), f'{model_path}/discriminator.pth')\n",
    "    \n",
    "    print(\"Models saved!\")\n",
    "    \n",
    "    return g_losses, d_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execution and Results\n",
    "\n",
    "Now we'll run the training function and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "g_losses, d_losses = train_gan(subset_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "Let's examine our final loss plot and generate some images with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(g_losses, d_losses, subset_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random samples after training\n",
    "with torch.no_grad():\n",
    "    # Generate random noise\n",
    "    z = torch.randn(16, latent_dim).to(device)\n",
    "    # Generate images\n",
    "    samples = generator(z).detach().cpu()\n",
    "    # Rescale images\n",
    "    samples = 0.5 * samples + 0.5\n",
    "    # Display images\n",
    "    grid = make_grid(samples, nrow=4, normalize=True)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Random Samples from Trained Generator\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps for Thesis Research\n",
    "\n",
    "1. **Data Subset Selection Strategies**:\n",
    "   - Random sampling (as baseline)\n",
    "   - Figure out a way to get informed subsest of the data\n",
    "\n",
    "2. **Evaluation Metrics**:\n",
    "   - Frechet Inception Distance (FID)\n",
    "   - Inception Score (IS)\n",
    "   - Precision and Recall metrics\n",
    "   - Training stability measures\n",
    "\n",
    "3. **Experiments to Run**:\n",
    "   - Train with different subset sizes (10%, 20%, 30%, etc.)\n",
    "   - Compare different subset selection methods\n",
    "   - Analyze what makes certain samples more \"informative\" for GANs\n",
    "   - Test if informative samples transfer across different GAN architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
